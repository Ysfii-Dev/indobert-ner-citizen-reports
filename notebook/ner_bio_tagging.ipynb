{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c87a7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load tokenizer dari model sentiment (agar tokenisasinya sesuai model BERT Bahasa Indonesia)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"taufiqdp/indonesian-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a79124d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LOC diperbarui, total entitas LOC: 7339\n",
      "[CLS]\tO\n",
      "lampu\tB-INFRA\n",
      "jalan\tI-INFRA\n",
      "di\tO\n",
      "jl\tB-LOC\n",
      ".\tI-LOC\n",
      "sudirman\tB-LOC\n",
      "mati\tB-PROB\n",
      "sejak\tO\n",
      "malam\tB-TIME\n",
      "kemarin\tB-TIME\n",
      "dan\tO\n",
      "kondisi\tO\n",
      "sangat\tO\n",
      "gelap\tB-DESC\n",
      "parah\tB-SEV\n",
      "[SEP]\tO\n",
      "‚úÖ Dataset CoNLL berhasil dibuat dengan FlashText + mapping entitas\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from flashtext import KeywordProcessor  # \n",
    "\n",
    "# Load tokenizer dari HuggingFace\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"taufiqdp/indonesian-sentiment\")\n",
    "\n",
    "# Kamus entitas diperluas (multi-token frasa juga)\n",
    "ENTITY_DICTIONARY = {\n",
    "    \"INFRA\": [\n",
    "        \"jalan\", \"jalan tol\", \"tol\", \"aspal\", \"trotoar\", \"jembatan\", \"flyover\", \"underpass\", \"rel\", \"lintasan\", \"busway\", \"pohon\", \"menara\",\n",
    "        \"halte\", \"terminal\", \"stasiun\", \"bandara\", \"pelabuhan\", \"gerbang\", \"pos\", \"troli\", \"tiang\", \"pagar\", \"tembok\", \"dinding\",\n",
    "        \"lampu\", \"penerangan\", \"lampu merah\", \"lampu jalan\", \"rambu\", \"traffic light\", \"marka\", \"pembatas\", \"batas\", \"monumen\", \"saluran air\", \"patung\",\n",
    "        \"drainase\", \"got\", \"selokan\", \"parit\", \"saluran\", \"irigasi\", \"bendungan\", \"embung\", \"waduk\",\n",
    "        \"sumur\", \"pompa\", \"pipa\", \"tandon\", \"manhole\", \"gorong-gorong\", \"rumah\", \"gedung\", \"kantor\", \"balai\", \"puskesmas\", \n",
    "        \"rumah sakit\", \"klinik\", \"sekolah\", \"kampus\", \"prasarana umum\",\"sarana\", \"prasarana\", \"prasda\",\n",
    "        \"masjid\", \"gereja\", \"wihara\", \"pura\", \"pos ronda\", \"balai desa\", \"fasilitas umum\"\n",
    "    ],\n",
    "    \"PROB\": [\n",
    "        \"rusak\", \"retak\", \"berlubang\", \"bolong\", \"hancur\", \"rapuh\", \"amblas\", \"terbakar\",\n",
    "        \"terbakar\", \"terbakar habis\", \"terbakar sebagian\", \"kebakran\", \"tumbang\", \"roboh\",\n",
    "        \"mati\", \"padam\", \"gelap\", \"terendam\", \"banjir\", \"macet\", \"tersumbat\", \"longsor\",\n",
    "        \"berkarat\", \"lapuk\", \"bocor\", \"kotor\", \"berdebu\", \"berisik\", \"bau\", \"bakar\", \"demo\",\n",
    "        \"patah\", \"hilang\", \"terpotong\", \"terkelupas\", \"mengelupas\", \"melebar\", \"runtuh\", \"ambruk\"\n",
    "    ],\n",
    "    \"LOC\": [\n",
    "        \"jakarta\", \"bandung\", \"medan\", \"surabaya\", \"semarang\", \"makassar\", \"denpasar\", \"yogyakarta\",\n",
    "        \"bogor\", \"depok\", \"tangerang\", \"bekasi\", \"malang\", \"padang\", \"pekanbaru\", \"palembang\", \"sidoarjo\", \"kebon siri raya\",\n",
    "        \"sudirman\", \"thamrin\", \"merdeka\", \"gatot\", \"subroto\", \"ahmad yani\", \"djuanda\", \"margonda\", \"senayan\",\n",
    "        \"diponegoro\", \"hayam wuruk\", \"cut nyak dien\", \"imam bonjol\",\n",
    "        \"utara\", \"selatan\", \"timur\", \"barat\", \"tengah\", \"pusat\", \"kec.\", \"kec\", \"kec.\", \"kel.\", \"kel\", \"kab.\", \"kab\",\n",
    "        \"jl.\", \"jl\", \"jln\", \"jln.\", \"jalan sudirman\", \"jalan thamrin\", \"jalan merdeka\", \"gatot subroto\", \"ahmad yani\",\n",
    "        \"jalan djuanda\", \"margonda raya\", \"polda\", \"polres\", \"polsek\", \"spbu\", \"pom bensin\", \"pom minyak\",\n",
    "        \"pasar\", \"perempatan\", \"simpang\", \"bundaran\", \"tugu\", \"alun-alun\", \"monas\", \"monumen nasional\", \"pajak\",\n",
    "        \"kelurahan\", \"kecamatan\", \"kabupaten\", \"desa\", \"daerah\", \"provinsi\", \"rt\", \"rw\", \"wilayah\", \"kota\"\n",
    "    ],\n",
    "    \"TIME\": [\n",
    "        \"kemarin\", \"tadi\", \"tadi malam\", \"tadi pagi\", \"siang\", \"malam\", \"subuh\", \"sore\",\n",
    "        \"bulan\", \"tahun\", \"hari\", \"besok\", \"lusa\", \"lalu\", \"baru saja\", \"sebentar\",\n",
    "        \"dinihari\", \"musim\", \"pekan\", \"akhir pekan\", \"hari ini\", \"setiap hari\", \"setiap minggu\",\"senin\",\"selasa\",\"rabu\",\"kamis\",\"jumat\",\"sabtu\", \"minggu\",\n",
    "        \"januari\", \"februari\", \"maret\", \"april\", \"mei\", \"juni\", \"juli\", \"agustus\", \"september\", \"oktober\", \"november\", \"desember\",\n",
    "        \"awal tahun\", \"pertengahan tahun\", \"akhir tahun\", \"awal bulan\", \"pertengahan bulan\", \"akhir bulan\",\n",
    "        \"pagi hari\", \"siang hari\", \"sore hari\", \"malam hari\", \"awal pekan\", \"pertengahan pekan\", \"akhir pekan\",\n",
    "        \"jam\", \"menit\", \"detik\", \"waktu\", \"periode\",\n",
    "        \"Jam 1\", \"Jam 2\", \"Jam 3\", \"Jam 4\", \"Jam 5\", \"Jam 6\", \"Jam 7\", \"Jam 8\", \"Jam 9\", \"Jam 10\", \"Jam 11\", \"Jam 12\",\n",
    "        \"WIB\", \"WITA\", \"WIT\"\n",
    "        \n",
    "        \n",
    "    ],\n",
    "    \"DESC\": [\n",
    "        \"bahaya\", \"berantakan\", \"ramai\", \"sepi\", \"gelap\", \"terang\", \"rawan\", \"licin\", \"padat\",\n",
    "        \"semrawut\", \"susah\", \"ribet\", \"membahayakan\", \"menakutkan\", \"mengganggu\", \"aman\", \"nyaman\",\n",
    "        \"kacau\", \"berisik\", \"kumuh\", \"jorok\", \"rapi\", \"tertib\", \"panas\", \"dekat\", \"jauh\", \"tinggi\", \"rendah\"\n",
    "    ],\n",
    "    \"SEV\": [\n",
    "        \"ringan\", \"sedang\", \"parah\", \"fatal\", \"berat\", \"kronis\", \"akut\", \"darurat\", \"gawat\",\n",
    "        \"kritis\", \"serius\", \"ekstrem\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# üîπ Tambahan: load LOC.csv dan gabungkan dengan LOC yang sudah ada\n",
    "try:\n",
    "    loc_df = pd.read_csv(\"LOC.csv\")\n",
    "    loc_list = loc_df[\"name\"].dropna().astype(str).str.lower().tolist()\n",
    "    ENTITY_DICTIONARY[\"LOC\"] = list(set(ENTITY_DICTIONARY[\"LOC\"] + loc_list))\n",
    "    print(f\"‚úÖ LOC diperbarui, total entitas LOC: {len(ENTITY_DICTIONARY['LOC'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Tidak bisa load LOC.csv: {e}\")\n",
    "\n",
    "#üîπ Siapkan FlashText + mapping keyword ‚Üí ent_type\n",
    "keyword_processor = KeywordProcessor(case_sensitive=False)\n",
    "keyword2type = {}\n",
    "\n",
    "for ent_type, keywords in ENTITY_DICTIONARY.items():\n",
    "    for kw in keywords:\n",
    "        kw_norm = kw.strip().lower()\n",
    "        if kw_norm:\n",
    "            keyword_processor.add_keyword(kw_norm)\n",
    "            keyword2type[kw_norm] = ent_type\n",
    "\n",
    "\n",
    "def label_sentence_with_flashtext(sentence, keyword_processor, keyword2type, tokenizer):\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_offsets_mapping=True,\n",
    "        return_attention_mask=False,\n",
    "        return_special_tokens_mask=False\n",
    "    )\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "    offsets = encoding[\"offset_mapping\"]\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    lowered = sentence.lower()\n",
    "\n",
    "    # Cari entitas dengan FlashText (keyword, start, end)\n",
    "    matches = keyword_processor.extract_keywords(lowered, span_info=True)\n",
    "\n",
    "    for ent, start_char, end_char in matches:\n",
    "        ent_type = keyword2type.get(ent.lower(), None)\n",
    "        if not ent_type:\n",
    "            continue\n",
    "\n",
    "        first = True\n",
    "        for idx, (off_start, off_end) in enumerate(offsets):\n",
    "            if off_start is None or off_end is None:\n",
    "                continue\n",
    "            if not (off_end <= start_char or off_start >= end_char):\n",
    "                if first:\n",
    "                    labels[idx] = f\"B-{ent_type}\"\n",
    "                    first = False\n",
    "                else:\n",
    "                    labels[idx] = f\"I-{ent_type}\"\n",
    "\n",
    "    # Validasi\n",
    "    assert len(tokens) == len(labels), f\"Mismatch: {len(tokens)} tokens vs {len(labels)} labels\"\n",
    "    return list(zip(tokens, labels))\n",
    "\n",
    "\n",
    "# üîπ Uji coba\n",
    "test = \"Lampu jalan di Jl. Sudirman mati sejak malam kemarin dan kondisi sangat gelap parah\"\n",
    "labeled = label_sentence_with_flashtext(test, keyword_processor, keyword2type, tokenizer)\n",
    "for tok, lab in labeled:\n",
    "    print(f\"{tok}\\t{lab}\")\n",
    "\n",
    "\n",
    "# üîπ Proses dataset full\n",
    "df = pd.read_csv(\"./preprocessed_data.csv\")\n",
    "all_sentences = []\n",
    "for sent in df[\"cleaned_text\"].dropna().tolist():\n",
    "    labeled = label_sentence_with_flashtext(sent, keyword_processor, keyword2type, tokenizer)\n",
    "    all_sentences.append(labeled)\n",
    "\n",
    "# üîπ Simpan ke CoNLL\n",
    "with open(\"./ner_dataset_convertedV3.conll\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sent in all_sentences:\n",
    "        for token, label in sent:\n",
    "            f.write(f\"{token}\\t{label}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"‚úÖ Dataset CoNLL berhasil dibuat dengan FlashText + mapping entitas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2becdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import pandas as pd\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # Load tokenizer dari HuggingFace\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"taufiqdp/indonesian-sentiment\")\n",
    "\n",
    "# # Kamus entitas diperluas (multi-token frasa juga)\n",
    "# ENTITY_DICTIONARY = {\n",
    "#     \"INFRA\": [\n",
    "#         \"jalan\", \"jalan tol\", \"aspal\", \"trotoar\", \"jembatan\", \"flyover\", \"underpass\", \"rel\", \"lintasan\",\n",
    "#         \"halte\", \"terminal\", \"stasiun\", \"bandara\", \"pelabuhan\", \"gerbang\", \"pos\", \"troli\", \"tiang\",\n",
    "#         \"lampu\", \"penerangan\", \"lampu merah\", \"lampu jalan\", \"rambu\", \"traffic light\", \"marka\", \"pembatas\", \"batas\",\n",
    "#         \"drainase\", \"got\", \"selokan\", \"parit\", \"saluran\", \"irigasi\", \"bendungan\", \"embung\", \"waduk\",\n",
    "#         \"sumur\", \"pompa\", \"pipa\", \"tandon\", \"manhole\", \"gorong-gorong\", \"rumah\", \"gedung\", \"kantor\", \"balai\", \"puskesmas\", \n",
    "#         \"rumah sakit\", \"klinik\", \"sekolah\", \"kampus\", \"prasarana umum\",\"sarana\", \"prasarana\", \"prasda\",\n",
    "#         \"masjid\", \"gereja\", \"wihara\", \"pura\", \"pos ronda\", \"balai desa\", \"fasilitas umum\"\n",
    "#     ],\n",
    "#     \"PROB\": [\n",
    "#         \"rusak\", \"retak\", \"berlubang\", \"bolong\", \"hancur\", \"rapuh\", \"amblas\", \"terbakar\",\n",
    "#         \"mati\", \"padam\", \"gelap\", \"terendam\", \"banjir\", \"macet\", \"tersumbat\", \"longsor\",\n",
    "#         \"berkarat\", \"lapuk\", \"bocor\", \"kotor\", \"berdebu\", \"berisik\", \"bau\", \"bakar\", \"demo\",\n",
    "#         \"patah\", \"hilang\", \"terpotong\", \"terkelupas\", \"mengelupas\", \"melebar\", \"runtuh\", \"ambruk\"\n",
    "#     ],\n",
    "#     \"LOC\": [\n",
    "#         \"jakarta\", \"bandung\", \"medan\", \"surabaya\", \"semarang\", \"makassar\", \"denpasar\", \"yogyakarta\",\n",
    "#         \"bogor\", \"depok\", \"tangerang\", \"bekasi\", \"malang\", \"padang\", \"pekanbaru\", \"palembang\", \"sidoarjo\", \"kebon siri raya\",\n",
    "#         \"sudirman\", \"thamrin\", \"merdeka\", \"gatot\", \"subroto\", \"ahmad yani\", \"djuanda\", \"margonda\",\n",
    "#         \"diponegoro\", \"hayam wuruk\", \"cut nyak dien\", \"imam bonjol\",\n",
    "#         \"jalan sudirman\", \"jalan thamrin\", \"jalan merdeka\", \"gatot subroto\", \"ahmad yani\",\n",
    "#         \"jalan djuanda\", \"margonda raya\", \"polda\", \"polres\", \"polsek\",\n",
    "#         \"kelurahan\", \"kecamatan\", \"kabupaten\", \"desa\", \"daerah\", \"provinsi\", \"rt\", \"rw\", \"wilayah\", \"kota\"\n",
    "#     ],\n",
    "#     \"TIME\": [\n",
    "#         \"kemarin\", \"tadi\", \"tadi malam\", \"tadi pagi\", \"siang\", \"malam\", \"subuh\", \"sore\",\n",
    "#         \"minggu\", \"bulan\", \"tahun\", \"hari\", \"besok\", \"lusa\", \"lalu\", \"baru saja\", \"sebentar\",\n",
    "#         \"dinihari\", \"musim\", \"pekan\", \"akhir pekan\", \"hari ini\", \"setiap hari\", \"setiap minggu\"\n",
    "#     ],\n",
    "#     \"DESC\": [\n",
    "#         \"bahaya\", \"berantakan\", \"ramai\", \"sepi\", \"gelap\", \"terang\", \"rawan\", \"licin\", \"padat\",\n",
    "#         \"semrawut\", \"susah\", \"ribet\", \"membahayakan\", \"menakutkan\", \"mengganggu\",\n",
    "#         \"kacau\", \"berisik\", \"kumuh\", \"jorok\", \"rapi\", \"tertib\", \"panas\", \"dekat\", \"jauh\", \"tinggi\", \"rendah\"\n",
    "#     ],\n",
    "#     \"SEV\": [\n",
    "#         \"ringan\", \"sedang\", \"parah\", \"fatal\", \"berat\", \"kronis\", \"akut\", \"darurat\", \"gawat\",\n",
    "#         \"kritis\", \"serius\", \"ekstrem\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# # üîπ Tambahan: load LOC.csv dan gabungkan dengan LOC yang sudah ada\n",
    "# try:\n",
    "#     loc_df = pd.read_csv(\"LOC.csv\")\n",
    "#     loc_list = loc_df[\"name\"].dropna().astype(str).str.lower().tolist()\n",
    "#     ENTITY_DICTIONARY[\"LOC\"] = list(set(ENTITY_DICTIONARY[\"LOC\"] + loc_list))\n",
    "#     print(f\"‚úÖ LOC diperbarui, total entitas LOC: {len(ENTITY_DICTIONARY['LOC'])}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ö†Ô∏è Tidak bisa load LOC.csv: {e}\")\n",
    "\n",
    "# # Persiapkan pola\n",
    "# ENTITY_PATTERNS = []\n",
    "# for ent_type, keywords in ENTITY_DICTIONARY.items():\n",
    "#     for kw in keywords:\n",
    "#         phrase = kw.strip().lower()\n",
    "#         if phrase:\n",
    "#             ENTITY_PATTERNS.append((phrase, ent_type))\n",
    "# ENTITY_PATTERNS.sort(key=lambda x: len(x[0].split()), reverse=True)\n",
    "\n",
    "\n",
    "# def label_sentence_with_bert_tokenizer(sentence, patterns, tokenizer):\n",
    "#     encoding = tokenizer(\n",
    "#         sentence,\n",
    "#         return_offsets_mapping=True,\n",
    "#         return_attention_mask=False,\n",
    "#         return_special_tokens_mask=False\n",
    "#     )\n",
    "#     tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "#     offsets = encoding[\"offset_mapping\"]\n",
    "\n",
    "#     labels = [\"O\"] * len(tokens)\n",
    "#     lowered = sentence.lower()\n",
    "\n",
    "#     for phrase, ent_type in patterns:\n",
    "#         for match in re.finditer(re.escape(phrase), lowered):\n",
    "#             start_char, end_char = match.start(), match.end()\n",
    "#             first = True\n",
    "#             for idx, (off_start, off_end) in enumerate(offsets):\n",
    "#                 if off_start is None or off_end is None:\n",
    "#                     continue\n",
    "#                 if not (off_end <= start_char or off_start >= end_char):\n",
    "#                     if first:\n",
    "#                         labels[idx] = f\"B-{ent_type}\"\n",
    "#                         first = False\n",
    "#                     else:\n",
    "#                         labels[idx] = f\"I-{ent_type}\"\n",
    "\n",
    "#     # üîπ Validasi tambahan\n",
    "#     # 1. Pastikan jumlah token == jumlah label\n",
    "#     assert len(tokens) == len(labels), f\"Mismatch: {len(tokens)} tokens vs {len(labels)} labels\"\n",
    "\n",
    "#     # 2. Pastikan tidak ada entity yang dimulai langsung dengan I-\n",
    "#     for i, lab in enumerate(labels):\n",
    "#         if lab.startswith(\"I-\") and (i == 0 or labels[i-1] == \"O\"):\n",
    "#             print(f\"‚ö†Ô∏è Warning: entity tanpa B- di token {tokens[i]} ({lab})\")\n",
    "\n",
    "#     return list(zip(tokens, labels))\n",
    "\n",
    "\n",
    "# # Uji coba\n",
    "# test = \"Lampu jalan di Jl. Sudirman mati sejak malam kemarin dan kondisi sangat gelap parah\"\n",
    "# labeled = label_sentence_with_bert_tokenizer(test, ENTITY_PATTERNS, tokenizer)\n",
    "# for tok, lab in labeled:\n",
    "#     print(f\"{tok}\\t{lab}\")\n",
    "\n",
    "\n",
    "# # Proses dataset\n",
    "# df = pd.read_csv(\"./preprocessed_data.csv\")\n",
    "# all_sentences = []\n",
    "# for sent in df[\"cleaned_text\"].dropna().tolist():\n",
    "#     labeled = label_sentence_with_bert_tokenizer(sent, ENTITY_PATTERNS, tokenizer)\n",
    "#     all_sentences.append(labeled)\n",
    "\n",
    "# # Simpan ke CoNLL\n",
    "# with open(\"./ner_dataset_convertedV2.conll\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for sent in all_sentences:\n",
    "#         for token, label in sent:\n",
    "#             f.write(f\"{token}\\t{label}\\n\")\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "# print(\"‚úÖ Dataset CoNLL berhasil dibuat dengan validasi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f623ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Kamus entitas diperluas (multi-token frasa juga)\n",
    "\n",
    "# ENTITY_DICTIONARY = {\n",
    "#     \"INFRA\": [\n",
    "#         # Transportasi & jalan\n",
    "#         \"jalan\", \"jalan tol\", \"aspal\", \"trotoar\", \"jembatan\", \"flyover\", \"underpass\", \"rel\", \"lintasan\",\n",
    "#         \"halte\", \"terminal\", \"stasiun\", \"bandara\", \"pelabuhan\", \"gerbang\", \"pos\", \"troli\",\n",
    "#         # Penerangan & lalu lintas\n",
    "#         \"lampu\", \"penerangan\", \"lampu merah\", \"lampu jalan\", \"rambu\", \"traffic light\", \"marka\",\n",
    "#         # Air & drainase\n",
    "#         \"drainase\", \"got\", \"selokan\", \"parit\", \"saluran\", \"irigasi\", \"bendungan\", \"embung\", \"waduk\",\n",
    "#         \"sumur\", \"pompa\", \"pipa\", \"tandon\", \"bak\", \"manhole\", \"gorong-gorong\",\n",
    "#         # Bangunan publik\n",
    "#         \"gedung\", \"kantor\", \"balai\", \"puskesmas\", \"rumah sakit\", \"klinik\", \"sekolah\", \"kampus\",\n",
    "#         \"masjid\", \"gereja\", \"wihara\", \"pura\", \"pos ronda\", \"balai desa\"\n",
    "#     ],\n",
    "\n",
    "#     \"PROB\": [\n",
    "#         \"rusak\", \"retak\", \"berlubang\", \"bolong\", \"hancur\", \"rapuh\", \"amblas\", \"terbakar\",\n",
    "#         \"mati\", \"padam\", \"gelap\", \"terendam\", \"banjir\", \"macet\", \"tersumbat\", \"longsor\",\n",
    "#         \"berkarat\", \"lapuk\", \"bocor\", \"kotor\", \"berdebu\", \"berisik\", \"bau\",\n",
    "#         \"patah\", \"hilang\", \"terpotong\", \"terkelupas\", \"mengelupas\", \"melebar\", \"runtuh\", \"ambruk\"\n",
    "#     ],\n",
    "\n",
    "#     \"LOC\": [\n",
    "#         # Kota besar\n",
    "#         \"jakarta\", \"bandung\", \"medan\", \"surabaya\", \"semarang\", \"makassar\", \"denpasar\", \"yogyakarta\",\n",
    "#         \"bogor\", \"depok\", \"tangerang\", \"bekasi\", \"malang\", \"padang\", \"pekanbaru\", \"palembang\",\n",
    "#         # Nama jalan umum\n",
    "#         \"sudirman\", \"thamrin\", \"merdeka\", \"gatot\", \"subroto\", \"ahmad yani\", \"djuanda\", \"margonda\",\n",
    "#         \"diponegoro\", \"hayam wuruk\", \"cut nyak dien\", \"imam bonjol\",\n",
    "#         \"jalan sudirman\", \"jalan thamrin\", \"jalan merdeka\", \"gatot subroto\", \"ahmad yani\",\n",
    "#         \"jalan djuanda\", \"margonda raya\"\n",
    "#         # Wilayah administrasi\n",
    "#         \"kelurahan\", \"kecamatan\", \"kabupaten\", \"desa\", \"provinsi\", \"rt\", \"rw\"\n",
    "#     ],\n",
    "\n",
    "#     \"TIME\": [\n",
    "#         \"kemarin\", \"tadi\", \"tadi malam\", \"tadi pagi\", \"siang\", \"malam\", \"subuh\", \"sore\",\n",
    "#         \"minggu\", \"bulan\", \"tahun\", \"hari\", \"besok\", \"lusa\", \"lalu\", \"baru saja\", \"sebentar\",\n",
    "#         \"dinihari\", \"musim\", \"pekan\", \"akhir pekan\", \"hari ini\", \"setiap hari\", \"setiap minggu\"\n",
    "#     ],\n",
    "\n",
    "#     \"DESC\": [\n",
    "#         \"bahaya\", \"berantakan\", \"ramai\", \"sepi\", \"gelap\", \"terang\", \"rawan\", \"licin\", \"padat\",\n",
    "#         \"semrawut\", \"susah\", \"ribet\", \"membahayakan\", \"menakutkan\", \"mengganggu\",\n",
    "#         \"kacau\", \"berisik\", \"kumuh\", \"jorok\", \"rapi\", \"tertib\"\n",
    "#     ],\n",
    "\n",
    "#     \"SEV\": [\n",
    "#         \"ringan\", \"sedang\", \"parah\", \"fatal\", \"berat\", \"kronis\", \"akut\", \"darurat\", \"gawat\",\n",
    "#         \"kritis\", \"serius\", \"ekstrem\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "\n",
    "# # Persiapkan pola (phrase, ent_type), dan urutkan berdasarkan jumlah kata (agar pencocokan multi-token dulu)\n",
    "# ENTITY_PATTERNS = []\n",
    "# for ent_type, keywords in ENTITY_DICTIONARY.items():\n",
    "#     for kw in keywords:\n",
    "#         # cleanup spasi ekstra & lowercase\n",
    "#         phrase = kw.strip().lower()\n",
    "#         if phrase:\n",
    "#             ENTITY_PATTERNS.append((phrase, ent_type))\n",
    "# # sort, frasa panjang (banyak kata) dulu\n",
    "# ENTITY_PATTERNS.sort(key=lambda x: len(x[0].split()), reverse=True)\n",
    "\n",
    "\n",
    "# def label_sentence_with_bert_tokenizer(sentence, patterns, tokenizer):\n",
    "#     \"\"\"\n",
    "#     Tokenisasi dengan tokenizer BERT ‚Üí menghasilkan input_ids + tokens (wordpiece),\n",
    "#     lalu cocokkan entitas multi-token di level kata ‚Üí ubah label ke token wordpiece.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Tokenisasi dengan output mapping ke kata asli\n",
    "#     encoding = tokenizer(sentence, return_offsets_mapping=True, return_attention_mask=False, return_special_tokens_mask=False)\n",
    "#     tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "#     offsets = encoding[\"offset_mapping\"]  # tiap token wordpiece punya (start, end) karakter dalam kalimat\n",
    "#     # offsets[0] dan offsets[-1] biasanya special tokens ([CLS], [SEP]) tergantung tokenizer\n",
    "\n",
    "#     labels = [\"O\"] * len(tokens)\n",
    "\n",
    "#     lowered = sentence.lower()\n",
    "\n",
    "#     for phrase, ent_type in patterns:\n",
    "#         # cari semua posisi kemunculan frasa (start char index) di teks\n",
    "#         for match in re.finditer(re.escape(phrase), lowered):\n",
    "#             start_char = match.start()\n",
    "#             end_char = match.end()\n",
    "#             # token-level: cari semua token whose offsets overlap dengan interval [start_char, end_char)\n",
    "#             # Kita beri label B-ent untuk token pertama overlapping, sisanya I-ent\n",
    "#             first = True\n",
    "#             for idx, (off_start, off_end) in enumerate(offsets):\n",
    "#                 if off_start is None or off_end is None:\n",
    "#                     continue\n",
    "#                 # cek overlap\n",
    "#                 if not (off_end <= start_char or off_start >= end_char):\n",
    "#                     if first:\n",
    "#                         labels[idx] = f\"B-{ent_type}\"\n",
    "#                         first = False\n",
    "#                     else:\n",
    "#                         labels[idx] = f\"I-{ent_type}\"\n",
    "\n",
    "#     # Pair token + label\n",
    "#     return list(zip(tokens, labels))\n",
    "\n",
    "\n",
    "# # Uji coba kecil\n",
    "# # contoh kalimat\n",
    "# test = \"Lampu jalan di Jl. Sudirman mati sejak malam kemarin dan kondisi sangat gelap parah\"\n",
    "# labeled = label_sentence_with_bert_tokenizer(test, ENTITY_PATTERNS, tokenizer)\n",
    "# for tok, lab in labeled:\n",
    "#     print(f\"{tok}\\t{lab}\")\n",
    "\n",
    "# # Kalau mau proses dataset:\n",
    "# df = pd.read_csv(\"./preprocessed_data.csv\")\n",
    "# all_sentences = []\n",
    "# for sent in df[\"cleaned_text\"].dropna().tolist():\n",
    "#     labeled = label_sentence_with_bert_tokenizer(sent, ENTITY_PATTERNS, tokenizer)\n",
    "#     all_sentences.append(labeled)\n",
    "\n",
    "# # Simpan ke CoNLL\n",
    "# with open(\"./ner_dataset_converted.conll\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for sent in all_sentences:\n",
    "#         for token, label in sent:\n",
    "#             f.write(f\"{token}\\t{label}\\n\")\n",
    "#         f.write(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
