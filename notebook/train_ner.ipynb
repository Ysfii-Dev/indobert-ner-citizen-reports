{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d360e4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Semester7\\NLP\\ner_proyek\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09855ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 74925 examples [00:00, 1895039.72 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '[CLS]\\tO'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Path ke dataset\n",
    "conll_file = r\"D:\\Semester7\\NLP\\ner_proyek\\ner_dataset_convertedV3.conll\"\n",
    "\n",
    "# Load dataset CoNLL ke HuggingFace Datasets\n",
    "raw_datasets = load_dataset(\"text\", data_files={\"train\": conll_file})\n",
    "\n",
    "# Cek contoh baris pertama\n",
    "print(raw_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8eaf697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh kalimat: ['[CLS]', 'sumpah', 'guy', '##ss', 'rusak', 'fasilitas', 'kita', '##a', 'pakai', 'rambu', 'lintas', 'km', '##ren', 'keli', '##st', 'rusak', '##k', 'wo', '##i', 'lampu', 'redup', 'bn', '##get', '##t', 'bahaya', '##a', 'kalo', 'berant', '##a', 'jalan', 'raya', 'nii', 'kondusif', 'orang', 'sibuk', 'yak', '##an', '[SEP]']\n",
      "Contoh label: ['O', 'O', 'O', 'O', 'B-PROB', 'O', 'O', 'O', 'O', 'B-INFRA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-INFRA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-INFRA', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Parsing CoNLL ke format HuggingFace\n",
    "def parse_conll(path):\n",
    "    sentences, labels = [], []\n",
    "    tokens, tags = [], []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:  # separator kalimat\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    labels.append(tags)\n",
    "                    tokens, tags = [], []\n",
    "                continue\n",
    "            token, label = line.split(\"\\t\")\n",
    "            tokens.append(token)\n",
    "            tags.append(label)\n",
    "    return sentences, labels\n",
    "\n",
    "# Parse file CoNLL\n",
    "sentences, labels = parse_conll(conll_file)\n",
    "\n",
    "print(\"Contoh kalimat:\", sentences[0])\n",
    "print(\"Contoh label:\", labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "801a1b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', 'sumpah', 'guy', '##ss', 'rusak', 'fasilitas', 'kita', '##a', 'pakai', 'rambu', 'lintas', 'km', '##ren', 'keli', '##st', 'rusak', '##k', 'wo', '##i', 'lampu', 'redup', 'bn', '##get', '##t', 'bahaya', '##a', 'kalo', 'berant', '##a', 'jalan', 'raya', 'nii', 'kondusif', 'orang', 'sibuk', 'yak', '##an', '[SEP]'], 'ner_tags': ['O', 'O', 'O', 'O', 'B-PROB', 'O', 'O', 'O', 'O', 'B-INFRA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-INFRA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-INFRA', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def parse_conll(path):\n",
    "    data = []\n",
    "    tokens, tags = [], []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    data.append({\"tokens\": tokens, \"ner_tags\": tags})\n",
    "                    tokens, tags = [], []\n",
    "                continue\n",
    "            token, label = line.split(\"\\t\")\n",
    "            tokens.append(token)\n",
    "            tags.append(label)\n",
    "    if tokens:  # sisa terakhir\n",
    "        data.append({\"tokens\": tokens, \"ner_tags\": tags})\n",
    "    return data\n",
    "\n",
    "# Parse file\n",
    "dataset_list = parse_conll(conll_file)\n",
    "\n",
    "# Konversi ke HuggingFace Dataset\n",
    "hf_dataset = Dataset.from_list(dataset_list)\n",
    "\n",
    "print(hf_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca17ccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 2617\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 327\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 328\n",
      "    })\n",
      "})\n",
      "{'tokens': ['[CLS]', 'user', 'rusak', 'indonesia', 'man', 'banjir', 'hutan', 'parah', 'gundul', 'orang', 'biang', 'ker', '##ok', 'buat', 'sempat', 'adil', 'pakai', 'hukum', 'jalan', '[SEP]'], 'ner_tags': ['O', 'O', 'B-PROB', 'O', 'O', 'B-PROB', 'O', 'B-SEV', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-INFRA', 'O']}\n"
     ]
    }
   ],
   "source": [
    "# Buat HuggingFace DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Split train/val/test (80/10/10)\n",
    "train_sents, test_sents, train_labels, test_labels = train_test_split(\n",
    "    sentences, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "val_sents, test_sents, val_labels, test_labels = train_test_split(\n",
    "    test_sents, test_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Buat HuggingFace Dataset\n",
    "train_dataset = Dataset.from_dict({\"tokens\": train_sents, \"ner_tags\": train_labels})\n",
    "val_dataset   = Dataset.from_dict({\"tokens\": val_sents, \"ner_tags\": val_labels})\n",
    "test_dataset  = Dataset.from_dict({\"tokens\": test_sents, \"ner_tags\": test_labels})\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "print(datasets)\n",
    "print(datasets[\"train\"][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a421d9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'B-DESC': 0, 'B-INFRA': 1, 'B-LOC': 2, 'B-PROB': 3, 'B-SEV': 4, 'B-TIME': 5, 'I-DESC': 6, 'I-INFRA': 7, 'I-LOC': 8, 'I-PROB': 9, 'O': 10}\n"
     ]
    }
   ],
   "source": [
    "# Encode Labels\n",
    "# Ambil semua label unik\n",
    "unique_tags = set(tag for tags in labels for tag in tags)\n",
    "label_list = sorted(list(unique_tags))\n",
    "\n",
    "# Mapping label â†” id\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(\"Label mapping:\", label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15cce1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2617/2617 [00:00<00:00, 2738.03 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 327/327 [00:00<00:00, 3767.27 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 328/328 [00:00<00:00, 3005.48 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', 'user', 'rusak', 'indonesia', 'man', 'banjir', 'hutan', 'parah', 'gundul', 'orang', 'biang', 'ker', '##ok', 'buat', 'sempat', 'adil', 'pakai', 'hukum', 'jalan', '[SEP]'], 'ner_tags': ['O', 'O', 'B-PROB', 'O', 'O', 'B-PROB', 'O', 'B-SEV', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-INFRA', 'O'], 'input_ids': [3, 3, 12287, 5236, 1718, 1781, 3726, 3283, 5695, 24247, 1646, 21117, 1678, 7, 7, 2623, 3815, 2941, 6187, 7563, 2156, 2050, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 10, 10, 3, 10, 10, 3, 10, 4, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1, 10, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenisasi & Align Labels\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer IndoBERT\n",
    "model_checkpoint = \"taufiqdp/indonesian-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Fungsi untuk align labels ke subword tokens\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)  # ignore loss\n",
    "            elif word_id != previous_word:\n",
    "                label_ids.append(label2id[label[word_id]])\n",
    "            else:\n",
    "                # Token lanjutan â†’ I-XXX kalau awalnya B-XXX\n",
    "                current_label = label2id[label[word_id]]\n",
    "                label_ids.append(current_label)\n",
    "            previous_word = word_id\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "print(tokenized_datasets[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90147c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at taufiqdp/indonesian-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([11, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True  # penting untuk ganti classification head\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d40c78bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics Evaluator\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2f1a43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.56.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2931118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_1804\\267522592.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "d:\\Semester7\\NLP\\ner_proyek\\myenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='820' max='820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [820/820 2:38:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.822900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.702000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.687100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.492100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.217700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.156700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.145200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.117800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.122400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Semester7\\NLP\\ner_proyek\\myenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=820, training_loss=0.3008243946040549, metrics={'train_runtime': 9552.747, 'train_samples_per_second': 1.37, 'train_steps_per_second': 0.086, 'total_flos': 854836566539520.0, 'train_loss': 0.3008243946040549, 'epoch': 5.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 16\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"ner-indobert\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "307dae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # ðŸ”¹ laporan detail per entitas\n",
    "    print(\"ðŸ“Š Laporan Per Entitas:\")\n",
    "    print(results)\n",
    "\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5ba7284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation...\n",
      "{'eval_loss': 0.13564752042293549, 'eval_precision': 0.8805409466566492, 'eval_recall': 0.915625, 'eval_f1': 0.8977403293757181, 'eval_accuracy': 0.9570147407483764, 'eval_runtime': 59.898, 'eval_samples_per_second': 5.476, 'eval_steps_per_second': 0.351, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "print(\"Evaluation...\")\n",
    "# Evaluation di test set (final evaluation)\n",
    "metrics = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31d717b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DESC     0.8043    0.9024    0.8506        41\n",
      "       INFRA     0.9284    0.9618    0.9448       445\n",
      "         LOC     0.6767    0.7344    0.7044       305\n",
      "        PROB     0.9879    1.0000    0.9939       407\n",
      "         SEV     0.9524    0.8000    0.8696        25\n",
      "        TIME     0.9333    0.9825    0.9573        57\n",
      "\n",
      "   micro avg     0.8805    0.9156    0.8977      1280\n",
      "   macro avg     0.8805    0.8969    0.8868      1280\n",
      "weighted avg     0.8841    0.9156    0.8992      1280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# ambil prediksi dari trainer\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# konversi id -> label (hilangkan -100)\n",
    "true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "true_predictions = [\n",
    "    [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "    for pred, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "# laporan per entitas\n",
    "print(classification_report(true_labels, true_predictions, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ad86c",
   "metadata": {},
   "source": [
    "Output pertama (classification report per entitas):\n",
    "\n",
    "* **DESC, INFRA, PROB, TIME** punya skor F1 tinggi (â‰¥0.85), artinya model sangat baik mendeteksi entitas ini.\n",
    "* **LOC** lebih rendah (F1 â‰ˆ0.70), berarti model cukup kesulitan mengenali lokasi dengan benar.\n",
    "* **SEV** hanya 25 data (support kecil), sehingga meskipun precision tinggi (0.95), recall rendah (0.80) membuat model kadang gagal mendeteksi semua entitas SEV.\n",
    "* **Rata-rata** (micro/macro/weighted) menunjukkan kinerja keseluruhan stabil (F1 â‰ˆ0.89), meski ada variasi antar-entitas.\n",
    "\n",
    "Output kedua (evaluasi global dari HuggingFace Trainer):\n",
    "\n",
    "* **Eval loss rendah (0.1356)** â†’ model fit dengan baik, tidak overfit.\n",
    "* **Precision 0.88, Recall 0.91, F1 0.89** â†’ konsisten dengan classification report.\n",
    "* **Accuracy 0.957** â†’ secara token-level, model mengklasifikasikan label dengan benar pada Â±96% token.\n",
    "\n",
    "Ringkasnya:\n",
    "Model NER bekerja sangat baik untuk sebagian besar entitas, terutama PROB, TIME, INFRA. Tantangan terbesar ada di **LOC** (mungkin karena variasi nama lokasi tinggi) dan **SEV** (karena data sedikit). Secara keseluruhan, model sudah **stabil dan handal (F1 global ~0.90, akurasi ~96%)**.\n",
    "\n",
    "Mau saya bikinkan juga saran **perbaikan spesifik untuk LOC dan SEV**?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "150926e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, dan trainer state saved to ./ner_indobert_model\n"
     ]
    }
   ],
   "source": [
    "# Save Model + Tokenizer + Trainer State\n",
    "save_dir = \"./ner_indobert_model\"\n",
    "\n",
    "# Simpan model + tokenizer (untuk inference / deployment)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "# Simpan juga trainer state (untuk resume training nanti)\n",
    "trainer.save_model(save_dir)\n",
    "\n",
    "print(f\"Model, tokenizer, dan trainer state saved to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17568eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Input: Jalan Sudirman di Jakarta mengalami kerusakan parah karena aspal jalan berlubang sejak minggu lalu, kondisi ini sangat membahayakan karena jalan menjadi licin dan gelap di malam hari.\n",
      "ðŸ”¹ Output NER:\n",
      "{'entity_group': 'INFRA', 'score': np.float32(0.9971186), 'word': 'jalan', 'start': 0, 'end': 5}\n",
      "{'entity_group': 'LOC', 'score': np.float32(0.7175946), 'word': 'sudirman', 'start': 6, 'end': 14}\n",
      "{'entity_group': 'LOC', 'score': np.float32(0.70571965), 'word': 'di', 'start': 15, 'end': 17}\n",
      "{'entity_group': 'LOC', 'score': np.float32(0.92562366), 'word': 'jakarta', 'start': 18, 'end': 25}\n",
      "{'entity_group': 'PROB', 'score': np.float32(0.74550533), 'word': 'mengalami', 'start': 26, 'end': 35}\n",
      "{'entity_group': 'PROB', 'score': np.float32(0.9138829), 'word': 'kerusakan', 'start': 36, 'end': 45}\n",
      "{'entity_group': 'SEV', 'score': np.float32(0.8121005), 'word': 'parah', 'start': 46, 'end': 51}\n",
      "{'entity_group': 'INFRA', 'score': np.float32(0.9969313), 'word': 'aspal', 'start': 59, 'end': 64}\n",
      "{'entity_group': 'INFRA', 'score': np.float32(0.99701154), 'word': 'jalan', 'start': 65, 'end': 70}\n",
      "{'entity_group': 'PROB', 'score': np.float32(0.9697356), 'word': 'berlubang', 'start': 71, 'end': 80}\n",
      "{'entity_group': 'TIME', 'score': np.float32(0.9741391), 'word': 'minggu', 'start': 87, 'end': 93}\n",
      "{'entity_group': 'TIME', 'score': np.float32(0.97010416), 'word': 'lalu', 'start': 94, 'end': 98}\n",
      "{'entity_group': 'SEV', 'score': np.float32(0.34254304), 'word': ',', 'start': 98, 'end': 99}\n",
      "{'entity_group': 'SEV', 'score': np.float32(0.35250962), 'word': 'kondisi', 'start': 100, 'end': 107}\n",
      "{'entity_group': 'SEV', 'score': np.float32(0.4002344), 'word': 'ini', 'start': 108, 'end': 111}\n",
      "{'entity_group': 'SEV', 'score': np.float32(0.45464364), 'word': 'sangat', 'start': 112, 'end': 118}\n",
      "{'entity_group': 'SEV', 'score': np.float32(0.46882337), 'word': 'membahayakan', 'start': 119, 'end': 131}\n",
      "{'entity_group': 'SEV', 'score': np.float32(0.37838998), 'word': 'karena', 'start': 132, 'end': 138}\n",
      "{'entity_group': 'INFRA', 'score': np.float32(0.9970203), 'word': 'jalan', 'start': 139, 'end': 144}\n",
      "{'entity_group': 'DESC', 'score': np.float32(0.34900278), 'word': 'menjadi', 'start': 145, 'end': 152}\n",
      "{'entity_group': 'DESC', 'score': np.float32(0.76617247), 'word': 'licin', 'start': 153, 'end': 158}\n",
      "{'entity_group': 'SEV', 'score': np.float32(0.35679874), 'word': 'dan', 'start': 159, 'end': 162}\n",
      "{'entity_group': 'DESC', 'score': np.float32(0.9204746), 'word': 'gelap', 'start': 163, 'end': 168}\n",
      "{'entity_group': 'DESC', 'score': np.float32(0.44326323), 'word': 'di', 'start': 169, 'end': 171}\n",
      "{'entity_group': 'TIME', 'score': np.float32(0.9416341), 'word': 'malam', 'start': 172, 'end': 177}\n",
      "{'entity_group': 'TIME', 'score': np.float32(0.94552225), 'word': 'hari', 'start': 178, 'end': 182}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load model & tokenizer dari folder simpanan\n",
    "inference_dir = \"./ner_indobert_model\"\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=inference_dir,\n",
    "    tokenizer=inference_dir,\n",
    "    aggregation_strategy=\"simple\"  # gabungkan multi-token entity (contoh: \"lampu jalan\" jadi 1 entitas)\n",
    ")\n",
    "\n",
    "# Contoh inference pada laporan masyarakat\n",
    "example_text = \"Jalan Sudirman di Jakarta mengalami kerusakan parah karena aspal jalan berlubang sejak minggu lalu, kondisi ini sangat membahayakan karena jalan menjadi licin dan gelap di malam hari.\"\n",
    "\n",
    "predictions = ner_pipeline(example_text)\n",
    "\n",
    "print(\"ðŸ”¹ Input:\", example_text)\n",
    "print(\"ðŸ”¹ Output NER:\")\n",
    "for pred in predictions:\n",
    "    print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b10b7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Input: Di Jalan Ahmad Yani, Bandung, sebuah pohon besar tumbang menimpa trotoar dan lampu jalan pada tadi malam, menyebabkan kondisi berbahaya karena area menjadi gelap dan sulit dilalui, dengan tingkat kerusakan cukup parah.\n",
      "ðŸ”¹ Output NER:\n",
      "{'entity_group': 'INFRA', 'score': np.float32(0.98606145), 'word': 'jalan', 'start': 3, 'end': 8}\n",
      "{'entity_group': 'LOC', 'score': np.float32(0.86693376), 'word': 'bandung', 'start': 21, 'end': 28}\n",
      "{'entity_group': 'INFRA', 'score': np.float32(0.9874736), 'word': 'sebuah', 'start': 30, 'end': 36}\n",
      "{'entity_group': 'INFRA', 'score': np.float32(0.9867684), 'word': 'pohon', 'start': 37, 'end': 42}\n",
      "{'entity_group': 'DESC', 'score': np.float32(0.6130531), 'word': 'besar', 'start': 43, 'end': 48}\n",
      "{'entity_group': 'PROB', 'score': np.float32(0.975183), 'word': 'tumbang', 'start': 49, 'end': 56}\n",
      "{'entity_group': 'SEV', 'score': np.float32(0.3671959), 'word': 'menimpa', 'start': 57, 'end': 64}\n",
      "{'entity_group': 'INFRA', 'score': np.float32(0.98611313), 'word': 'trotoar', 'start': 65, 'end': 72}\n",
      "{'entity_group': 'INFRA', 'score': np.float32(0.9827717), 'word': 'lampu', 'start': 77, 'end': 82}\n",
      "{'entity_group': 'INFRA', 'score': np.float32(0.98254365), 'word': 'jalan', 'start': 83, 'end': 88}\n",
      "{'entity_group': 'TIME', 'score': np.float32(0.4813318), 'word': 'pada', 'start': 89, 'end': 93}\n",
      "{'entity_group': 'TIME', 'score': np.float32(0.7708239), 'word': 'tadi', 'start': 94, 'end': 98}\n",
      "{'entity_group': 'TIME', 'score': np.float32(0.95281273), 'word': 'malam', 'start': 99, 'end': 104}\n",
      "{'entity_group': 'DESC', 'score': np.float32(0.54310966), 'word': ',', 'start': 104, 'end': 105}\n",
      "{'entity_group': 'DESC', 'score': np.float32(0.6548224), 'word': 'menyebabkan', 'start': 106, 'end': 117}\n",
      "{'entity_group': 'DESC', 'score': np.float32(0.817583), 'word': 'berbahaya', 'start': 126, 'end': 135}\n",
      "{'entity_group': 'DESC', 'score': np.float32(0.4756994), 'word': 'karena', 'start': 136, 'end': 142}\n",
      "{'entity_group': 'DESC', 'score': np.float32(0.46474242), 'word': 'area', 'start': 143, 'end': 147}\n",
      "{'entity_group': 'DESC', 'score': np.float32(0.49086735), 'word': 'menjadi', 'start': 148, 'end': 155}\n",
      "{'entity_group': 'DESC', 'score': np.float32(0.893147), 'word': 'gelap', 'start': 156, 'end': 161}\n",
      "{'entity_group': 'DESC', 'score': np.float32(0.47969478), 'word': 'dan', 'start': 162, 'end': 165}\n",
      "{'entity_group': 'DESC', 'score': np.float32(0.7428707), 'word': 'sulit', 'start': 166, 'end': 171}\n",
      "{'entity_group': 'DESC', 'score': np.float32(0.63332397), 'word': 'dilalui', 'start': 172, 'end': 179}\n",
      "{'entity_group': 'SEV', 'score': np.float32(0.5334517), 'word': 'dengan', 'start': 181, 'end': 187}\n",
      "{'entity_group': 'SEV', 'score': np.float32(0.5304712), 'word': 'tingkat', 'start': 188, 'end': 195}\n",
      "{'entity_group': 'PROB', 'score': np.float32(0.49578378), 'word': 'kerusakan', 'start': 196, 'end': 205}\n",
      "{'entity_group': 'SEV', 'score': np.float32(0.56043077), 'word': 'cukup', 'start': 206, 'end': 211}\n",
      "{'entity_group': 'SEV', 'score': np.float32(0.561968), 'word': 'parah', 'start': 212, 'end': 217}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load model & tokenizer dari folder simpanan\n",
    "inference_dir = \"./ner_indobert_model\"\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=inference_dir,\n",
    "    tokenizer=inference_dir,\n",
    "    aggregation_strategy=\"simple\"  \n",
    ")\n",
    "\n",
    "# Contoh inference pada laporan masyarakat\n",
    "example_text = \"Di Jalan Ahmad Yani, Bandung, sebuah pohon besar tumbang menimpa trotoar dan lampu jalan pada tadi malam, menyebabkan kondisi berbahaya karena area menjadi gelap dan sulit dilalui, dengan tingkat kerusakan cukup parah.\"\n",
    "\n",
    "predictions = ner_pipeline(example_text)\n",
    "\n",
    "print(\"ðŸ”¹ Input:\", example_text)\n",
    "print(\"ðŸ”¹ Output NER:\")\n",
    "for pred in predictions:\n",
    "    print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0ca96d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Input: ada pohon tumabang di depan spbu Gatot Subroto pagi hari\n",
      "ðŸ”¹ Output NER:\n",
      "{'entity_group': 'INFRA', 'score': np.float32(0.9918521), 'word': 'pohon', 'start': 4, 'end': 9}\n",
      "{'entity_group': 'LOC', 'score': np.float32(0.88788754), 'word': 'depan', 'start': 22, 'end': 27}\n",
      "{'entity_group': 'LOC', 'score': np.float32(0.63920444), 'word': 'spbu gatot subroto', 'start': 28, 'end': 46}\n",
      "{'entity_group': 'TIME', 'score': np.float32(0.7546842), 'word': 'pagi', 'start': 47, 'end': 51}\n",
      "{'entity_group': 'TIME', 'score': np.float32(0.9845931), 'word': 'hari', 'start': 52, 'end': 56}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load model & tokenizer dari folder simpanan\n",
    "inference_dir = \"./ner_indobert_model\"\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=inference_dir,\n",
    "    tokenizer=inference_dir,\n",
    "    aggregation_strategy=\"simple\"  \n",
    ")\n",
    "\n",
    "# Contoh inference pada laporan masyarakat\n",
    "example_text = \"ada pohon tumabang di depan spbu Gatot Subroto pagi hari\"\n",
    "\n",
    "predictions = ner_pipeline(example_text)\n",
    "\n",
    "print(\"ðŸ”¹ Input:\", example_text)\n",
    "print(\"ðŸ”¹ Output NER:\")\n",
    "for pred in predictions:\n",
    "    print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca02e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
